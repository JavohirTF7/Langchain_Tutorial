{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d081f81c-9c27-4739-ba79-01c5153c4bee",
   "metadata": {},
   "source": [
    "# FILE loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f6cfa-10ac-499e-bdd2-bbf0fbba70c1",
   "metadata": {},
   "source": [
    "**docx loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db694bc7-edb8-4c1a-aaf0-d540557b4e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import Docx2txtLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af3fa3-2dea-4f5c-806a-165b7c5b3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loader = Docx2txtLoader(\"docs/op.docx\")\n",
    "docx=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec869630-1fd5-4814-b547-d57e009a0940",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03837979-989c-4790-a581-196237a4625d",
   "metadata": {},
   "source": [
    "* **pdf loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ed495-ade5-48db-a430-b2a6a8864c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773e8c6-72f3-4696-836e-f809d4dd1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"docs/mexanika.pdf\")\n",
    "data=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854e7c1-b4a9-4be8-98f9-4dc8fa69f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347b678-a489-4d2d-a527-a72ce8fdbd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[22].page_content)} characters in your document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1defcdb9-5856-4eda-9e94-3be1de5e1a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[2].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4250b1-24f0-4fe0-b44e-4ec94e75cb7c",
   "metadata": {},
   "source": [
    "# Pdf, docx, text loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b48773e-fc6d-41bb-97b0-93552e230111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(file):\n",
    "    \n",
    "    documents = []\n",
    "    for file in os.listdir(\"docs\"):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdf_path = \"./docs/\" + file\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "        elif file.endswith('.docx') or file.endswith('.doc'):\n",
    "            doc_path = \"./docs/\" + file\n",
    "            loader = Docx2txtLoader(doc_path)\n",
    "            documents.extend(loader.load())\n",
    "        elif file.endswith('.txt'):\n",
    "            text_path = \"./docs/\" + file\n",
    "            loader = TextLoader(text_path)\n",
    "            documents.extend(loader.load())\n",
    "        return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077dea56-dc24-4e1e-82f7-86957a0e0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e37d7-7682-4875-92bf-9ad8641f56d1",
   "metadata": {},
   "source": [
    "* **Text downloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00036f0d-9028-4245-91d2-24ecfe1f0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    " \n",
    "# Load text data from a file using TextLoader\n",
    "loader = TextLoader(\"./data/sample.txt\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3f90f-d0de-4351-b964-3e6e0d891678",
   "metadata": {},
   "source": [
    "* **web page downloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3642e7-c974-41c3-a4e1-a43436e89c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03faf98-7e4d-4d65-9aa9-e93e4bdaecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = WebBaseLoader(link_list)\n",
    "docs = loader.load()\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2431940-a3ac-418e-8064-6a9e2821f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'You have {len(docs)} document(s) in your data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe6405-ff3e-493a-8614-2d0a07f42dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = \"https://kun.uz/uz/news/2023/09/17/mening-yolim-xushxabari-rossiyalik-deputatga-javoblar-va-konvertatsiya-shov-shuvi-hafta-dayjesti\"\n",
    "\n",
    "# Barcha matnlar uchun to'plangan ro'yxat\n",
    "all_matnlar = []\n",
    "\n",
    "for url in urls:\n",
    "    # Web sahifani HTML ma'lumotlarini olish\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # BeautifulSoup kutubxonasini ishlatib HTML-ni parsing qilish\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Matn elementlarini izlash va ro'yxatga qo'shish\n",
    "    matnlar = soup.find_all('p')  # Misol uchun <p> teglaridagi matnlarni izlaymiz\n",
    "    all_matnlar.extend(matnlar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13c6e7-573f-4469-9577-d76a259ccb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Web sahifani URL manzili\n",
    "url = 'https://kun.uz/uz/news/2023/09/17/mening-yolim-xushxabari-rossiyalik-deputatga-javoblar-va-konvertatsiya-shov-shuvi-hafta-dayjesti'\n",
    "\n",
    "# Web sahifani HTML ma'lumotlarini olish\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# BeautifulSoup kutubxonasini ishlatib HTML-ni parsing qilish\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Matn elementlarini izlash va chiqarish\n",
    "matnlar = soup.find_all('p')  # Misol uchun <p> teglaridagi matnlarni izlaymiz\n",
    "\n",
    "for matn in matnlar:\n",
    "    print(matn.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186ceb3-711d-48c2-b91e-901d0a19c969",
   "metadata": {},
   "source": [
    "# ALL links in pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb3537-a317-422e-91da-851db4921cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "# function to extract html document from given url\n",
    "def getHTMLdocument(url):\n",
    "      \n",
    "    # request for HTML document of given url\n",
    "    response = requests.get(url)\n",
    "      \n",
    "    # response will be provided in JSON format\n",
    "    return response.text\n",
    "  \n",
    "    \n",
    "# assign required credentials\n",
    "# assign URL\n",
    "url_to_scrape = \"https://kun.uz/uz\"\n",
    "  \n",
    "# create document\n",
    "html_document = getHTMLdocument(url_to_scrape)\n",
    "  \n",
    "# create soap object\n",
    "soup = BeautifulSoup(html_document, 'html.parser')\n",
    "  \n",
    "  \n",
    "# find all the anchor tags with \"href\" \n",
    "# attribute starting with \"https://\"\n",
    "for link in soup.find_all('a', \n",
    "                          attrs={'href': re.compile(\"^https://\")}):\n",
    "    # display the actual urls\n",
    "    print(link.get('href'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc6ab6-6865-4f8b-b044-bf92581695fb",
   "metadata": {},
   "source": [
    "# All links in web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94490710-5118-4faa-b5d4-6003bc1cc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_all_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    base_url = response.url\n",
    "\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            absolute_url = urljoin(base_url, href)\n",
    "            links.append(absolute_url)\n",
    "    return links\n",
    "\n",
    "# Veb sahifaning URL manzili\n",
    "url = 'https://kun.uz/uz'\n",
    "\n",
    "# Barcha linklarni olish\n",
    "links = get_all_links(url)\n",
    "\n",
    "# Natijalarni chiqarish\n",
    "for link in links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097471c-c0b5-4e61-87d6-fcbe8b80e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_all_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    base_url = response.url\n",
    "\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            absolute_url = urljoin(base_url, href)\n",
    "            if absolute_url.startswith('https://kun.uz'):\n",
    "                links.append(absolute_url)\n",
    "    return links\n",
    "\n",
    "# Web sitesinin URL adresi\n",
    "url = 'https://kun.uz/uz'\n",
    "\n",
    "# Tüm bağlantıları al\n",
    "links = get_all_links(url)\n",
    "\n",
    "# Sonuçları yazdır\n",
    "for link in links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b799f-71c7-43fd-9d4d-5562e56c13ad",
   "metadata": {},
   "source": [
    "# Looks for links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d98567c-9495-4c1e-a1dd-876884a2a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_all_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    base_url = response.url\n",
    "\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            absolute_url = urljoin(base_url, href)\n",
    "            links.append(absolute_url)\n",
    "    return links\n",
    "\n",
    "def scan_website(url):\n",
    "    scanned_links = set()\n",
    "    links_to_scan = set([url])\n",
    "\n",
    "    while links_to_scan:\n",
    "        current_url = links_to_scan.pop()\n",
    "        if current_url in scanned_links:\n",
    "            continue\n",
    "\n",
    "        print(\"Scanning:\", current_url)\n",
    "\n",
    "        try:\n",
    "            links = get_all_links(current_url)\n",
    "            scanned_links.add(current_url)\n",
    "            links_to_scan.update(links)\n",
    "        except Exception as e:\n",
    "            print(\"Error scanning\", current_url, \":\", str(e))\n",
    "\n",
    "    return scanned_links\n",
    "\n",
    "# Veb sahifaning URL manzili\n",
    "url = 'https://championat.asia/'\n",
    "\n",
    "# Veb saytni skanlash\n",
    "scanned_links = scan_website(url)\n",
    "\n",
    "# Natijalarni chiqarish\n",
    "for link in scanned_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df704e-0ef3-40ea-80e1-ca23ce9f3766",
   "metadata": {},
   "source": [
    "**ALL links in web site**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223fe4e-e158-40b9-8cbc-bcae47146659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_all_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    base_url = response.url\n",
    "\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            absolute_url = urljoin(base_url, href)\n",
    "            links.append(absolute_url)\n",
    "    return links\n",
    "\n",
    "def scan_website(url):\n",
    "    scanned_links = set()\n",
    "    links_to_scan = set([url])\n",
    "\n",
    "    while links_to_scan:\n",
    "        current_url = links_to_scan.pop()\n",
    "        if current_url in scanned_links:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            links = get_all_links(current_url)\n",
    "            scanned_links.add(current_url)\n",
    "            links_to_scan.update(links)\n",
    "        except Exception as e:\n",
    "            print(\"Error scanning\", current_url, \":\", str(e))\n",
    "\n",
    "    return scanned_links\n",
    "\n",
    "# Veb sahifaning URL manzili\n",
    "url = 'https://www.bbc.com/uzbek'\n",
    "\n",
    "# Veb saytni skanlash\n",
    "scanned_links = scan_website(url)\n",
    "\n",
    "# Natijalarni listga saqlash\n",
    "link_list = list(scanned_links)\n",
    "\n",
    "# Natijalarni chiqarish\n",
    "print(link_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0716e3-c102-470e-b32c-33387611f99e",
   "metadata": {},
   "source": [
    "# first 20 links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fc49b5f-3ba2-42eb-b4b4-0af87e91ce03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.bbc.com/uzbek/uzbekistan-66806503', 'https://www.bbc.com/uzbek/uzbekistan-66397404', 'https://www.bbc.com/uzbek/world-66777204', 'https://www.bbc.com/uzbek/topics/cl8l9mved19t', 'https://www.bbc.com/uzbek/uzbekistan-65636164', 'https://www.bbc.com/uzbek/topics/cqywjwj4k84t', 'https://www.bbc.com/uzbek/world-66613321', 'https://www.bbc.com/uzbek/uzbekistan-66645150', 'https://www.bbc.com/uzbek/world-63575423', 'https://www.bbc.com/uzbek/topics/cqm6qnememet', 'https://www.bbc.com/uzbek/topics/cqywjwj4k84t?page=2', 'https://www.bbc.com/uzbek/uzbekistan-64220622', 'https://www.bbc.co.uk/editorialguidelines/guidance/feeds-and-links', 'https://www.bbc.com/uzbek/world-63998351', 'https://www.bbc.com/uzbek/topics/cl8l9mved19t?page=3', 'https://www.bbc.com/uzbek/topics/cqywjwj4k84t?page=3', 'https://www.bbc.com/uzbek/uzbekistan-55588499', 'https://www.bbc.com/uzbek/uzbekistan', 'https://www.bbc.com/uzbek/topics/cvjp2p2n1jxt', 'https://www.bbc.com/uzbek/uzbekistan-66824029']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_all_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    base_url = response.url\n",
    "\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            absolute_url = urljoin(base_url, href)\n",
    "            links.append(absolute_url)\n",
    "    return links\n",
    "\n",
    "def scan_website(url, limit=100):\n",
    "    scanned_links = set()\n",
    "    links_to_scan = set([url])\n",
    "\n",
    "    while links_to_scan and len(scanned_links) < limit:\n",
    "        current_url = links_to_scan.pop()\n",
    "        if current_url in scanned_links:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            links = get_all_links(current_url)\n",
    "            scanned_links.add(current_url)\n",
    "            links_to_scan.update(links)\n",
    "        except Exception as e:\n",
    "            print(\"Error scanning\", current_url, \":\", str(e))\n",
    "\n",
    "    return scanned_links\n",
    "\n",
    "# Veb sahifaning URL manzili\n",
    "url = 'https://www.bbc.com/uzbek/uzbekistan'\n",
    "\n",
    "# Veb saytni skanlash va faqat dastlabki 100 tasini olish\n",
    "scanned_links = scan_website(url, limit=20)\n",
    "\n",
    "# Natijalarni listga saqlash\n",
    "link_list = list(scanned_links)\n",
    "\n",
    "# Natijalarni chiqarish\n",
    "print(link_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b0ea7-e864-4758-a125-e8ae369be812",
   "metadata": {},
   "source": [
    "# Links date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e61c7a04-3257-4acb-be68-159f69467241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-14\n",
      "2023-08-03\n",
      "2023-09-11\n",
      "2023-09-18\n",
      "2023-05-18\n",
      "2023-08-03\n",
      "2023-08-27\n",
      "2023-08-29\n",
      "2022-11-10\n",
      "2023-09-18\n",
      "2021-11-15\n",
      "2023-01-11\n",
      "2019-07-04\n",
      "2022-12-16\n",
      "2023-08-22\n",
      "2021-01-27\n",
      "2021-01-08\n",
      "2023-09-18\n",
      "2023-08-18\n",
      "2023-09-15\n"
     ]
    }
   ],
   "source": [
    "for url in link_list:\n",
    "     # URL manzili\n",
    "    date = find_date(url)\n",
    "    print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f5e6f1f-7f45-4199-b596-c6e8d46163b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-09-15'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b669331-bbdb-4347-8dd2-e2b4e27dd049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-09-14'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from htmldate import find_date\n",
    "find_date('https://www.bbc.com/uzbek/uzbekistan-66806503')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c6886-5c16-443f-91d9-42e45408aee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed7e01-c6e0-4758-ad8a-d13aad015ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4bb7e-a2eb-496c-aca8-badfc93acab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07801ee3-bfb5-4556-9ab6-31f428ad2d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
